{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BY3SiEE5d35L",
        "outputId": "93274adb-d9b0-437b-e049-56a565253fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.10/dist-packages (0.70.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.66.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2024.9.11)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (3.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.13.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.17.0)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.6.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.2.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.19.1)\n",
            "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.18.5)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.39.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.2.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.3)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.4.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (75.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2024.8.30)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->simpletransformers) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.10.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2024.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->simpletransformers) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.5.0)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (10.4.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.3.3)\n",
            "Requirement already satisfied: watchdog<6,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.0.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.7)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->simpletransformers) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install simpletransformers scikit-learn xgboost optuna\n",
        "\n",
        "import pandas as pd\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "import logging\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "import optuna\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm4ePXPTsYcI",
        "outputId": "00fb1f68-25bc-4b6e-9987-cbc7d473c2e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging to capture outputs\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "# Load datasets\n",
        "olid_train = pd.read_csv('/content/drive/MyDrive/sem4/sem4/olid-train-small.csv')\n",
        "hasoc_train = pd.read_csv('/content/drive/MyDrive/sem4/sem4/hasoc-train.csv')\n",
        "olid_test = pd.read_csv('/content/drive/MyDrive/sem4/sem4/olid-test.csv')\n",
        "\n",
        "# Prepare OLID and HASOC train data\n",
        "olid_train_data = pd.DataFrame({\n",
        "    'text': olid_train['text'],\n",
        "    'labels': olid_train['labels']\n",
        "})\n",
        "\n",
        "hasoc_train_data = pd.DataFrame({\n",
        "    'text': hasoc_train['text'],\n",
        "    'labels': hasoc_train['labels']\n",
        "})\n",
        "\n",
        "# Prepare OLID test data\n",
        "olid_test_data = pd.DataFrame({\n",
        "    'text': olid_test['text'],\n",
        "    'labels': olid_test['labels']\n",
        "})\n",
        "\n",
        "# In-domain Models\n",
        "roberta = \"/content/drive/MyDrive/results/OLID_roberta-base_lr_2e-05_bs_16_epochs_5\"\n",
        "bert = \"/content/drive/MyDrive/results/OLID_bert-base-cased_lr_1e-05_bs_16_epochs_5\"\n",
        "hatebert = \"/content/drive/MyDrive/results/OLID_GroNLP/hateBERT_lr_1e-05_bs_16_epochs_5\"\n",
        "distrillbert = \"/content/drive/MyDrive/results/OLID_distilbert-base-cased_lr_1e-05_bs_16_epochs_5\"\n",
        "# xlnet = \"\"\n",
        "\n",
        "# Cross-domain Models\n",
        "cross_roberta = \"/content/drive/MyDrive/results/HASOC_roberta-base_lr_1e-05_bs_16_epochs_5\"\n",
        "cross_bert = \"/content/drive/MyDrive/results/HASOC_bert-base-cased_lr_2e-05_bs_16_epochs_5\"\n",
        "cross_hatebert = \"/content/drive/MyDrive/results/HASOC_GroNLP/hateBERT_lr_1e-05_bs_16_epochs_5\"\n",
        "cross_distrillbert = \"/content/drive/MyDrive/results/HASOC_distilbert-base-cased_lr_1e-05_bs_16_epochs_5\"\n",
        "# xlnet_cross = \"\"\n",
        "\n",
        "# Declare the in-domain stacks to try\n",
        "in_domain_stack_one = [roberta, bert, hatebert]\n",
        "in_domain_stack_two = [roberta, distrillbert, bert]\n",
        "\n",
        "# Declare the cross domains stacks to try\n",
        "cross_domain_stack_one = [cross_roberta, cross_bert, cross_hatebert]\n",
        "cross_domain_stack_two = [cross_roberta, cross_distrillbert, cross_bert]"
      ],
      "metadata": {
        "id": "Nk1Iu60Td-dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Optuna objective function based on the meta-model type\n",
        "def objective(trial, meta_model_type, meta_features, y_train):\n",
        "    if meta_model_type == 'logistic':\n",
        "        # Suggest hyperparameters for Logistic Regression\n",
        "        C = trial.suggest_loguniform('C', 1e-3, 1e2)\n",
        "        penalty = trial.suggest_categorical('penalty', ['l2', None])\n",
        "        solver = trial.suggest_categorical('solver', ['lbfgs', 'saga'])\n",
        "        meta_model = LogisticRegression(C=C, penalty=penalty, solver=solver)\n",
        "\n",
        "    elif meta_model_type == 'xgboost':\n",
        "        # Suggest hyperparameters for XGBoost\n",
        "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
        "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
        "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-3, 1e-1)\n",
        "        subsample = trial.suggest_uniform('subsample', 0.5, 1.0)\n",
        "        colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.5, 1.0)\n",
        "        meta_model = xgb.XGBClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            learning_rate=learning_rate,\n",
        "            subsample=subsample,\n",
        "            colsample_bytree=colsample_bytree,\n",
        "            use_label_encoder=False\n",
        "        )\n",
        "\n",
        "    elif meta_model_type == 'random_forest':\n",
        "        # Suggest hyperparameters for RandomForestClassifier\n",
        "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
        "        max_depth = trial.suggest_int('max_depth', 5, 20)\n",
        "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
        "        meta_model = RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_samples_leaf=min_samples_leaf\n",
        "        )\n",
        "\n",
        "    # Perform cross-validation on the meta-model\n",
        "    f1_scores = cross_val_score(meta_model, meta_features, y_train, cv=5, scoring='f1')\n",
        "    return f1_scores.mean()"
      ],
      "metadata": {
        "id": "F9a3uApK7X65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import joblib\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "import optuna\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the Optuna objective function for hyperparameter optimization\n",
        "def objective(trial, meta_model_type, meta_features, y_train):\n",
        "    if meta_model_type == 'logistic':\n",
        "        C = trial.suggest_loguniform('C', 1e-3, 1e2)\n",
        "        penalty = trial.suggest_categorical('penalty', ['l2', None])\n",
        "        solver = trial.suggest_categorical('solver', ['lbfgs', 'saga'])\n",
        "        meta_model = LogisticRegression(C=C, penalty=penalty, solver=solver)\n",
        "    elif meta_model_type == 'xgboost':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
        "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
        "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-3, 1e-1)\n",
        "        subsample = trial.suggest_uniform('subsample', 0.5, 1.0)\n",
        "        colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.5, 1.0)\n",
        "        meta_model = xgb.XGBClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            learning_rate=learning_rate,\n",
        "            subsample=subsample,\n",
        "            colsample_bytree=colsample_bytree,\n",
        "            use_label_encoder=False\n",
        "        )\n",
        "    elif meta_model_type == 'random_forest':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
        "        max_depth = trial.suggest_int('max_depth', 5, 20)\n",
        "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
        "        meta_model = RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_samples_leaf=min_samples_leaf\n",
        "        )\n",
        "    # Perform cross-validation on the meta-model\n",
        "    f1_scores = cross_val_score(meta_model, meta_features, y_train, cv=5, scoring='f1')\n",
        "    return f1_scores.mean()\n",
        "\n",
        "# Main function to perform stacking ensemble with Optuna optimization\n",
        "def stacking_ensemble_with_optuna(models, train_data, eval_data, meta_model_type='logistic', k_folds=5, save_path=\"\"):\n",
        "    save_path = f\"{save_path}_{meta_model_type}_optuna\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    X_train = np.array(train_data['text'])\n",
        "    y_train = np.array(train_data['labels'])\n",
        "    X_eval = np.array(eval_data['text'])\n",
        "    y_eval = np.array(eval_data['labels'])\n",
        "\n",
        "    meta_features = np.zeros((len(train_data), len(models)))\n",
        "    meta_eval_features = np.zeros((len(eval_data), len(models)))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
        "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
        "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
        "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "        for i, model_path in enumerate(models):\n",
        "            model_type = 'roberta' if 'roberta' in model_path else 'distilbert' if 'distilbert' in model_path else 'bert'\n",
        "            print(f\"Loading and evaluating model: {model_path} (model type: {model_type})\")\n",
        "            model = ClassificationModel(model_type, model_path, use_cuda=True)\n",
        "\n",
        "            fold_val_data = pd.DataFrame({'text': X_fold_val, 'labels': y_fold_val})\n",
        "            _, model_outputs, _ = model.eval_model(fold_val_data)\n",
        "            meta_features[val_idx, i] = model_outputs[:, 1]\n",
        "\n",
        "            _, model_eval_outputs, _ = model.eval_model(eval_data)\n",
        "            meta_eval_features[:, i] += model_eval_outputs[:, 1] / k_folds\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda trial: objective(trial, meta_model_type, meta_features, y_train), n_trials=20)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    print(f\"Best hyperparameters found by Optuna for {meta_model_type}: {best_params}\")\n",
        "\n",
        "    if meta_model_type == 'logistic':\n",
        "        meta_model = LogisticRegression(**best_params)\n",
        "    elif meta_model_type == 'xgboost':\n",
        "        meta_model = xgb.XGBClassifier(**best_params, use_label_encoder=False)\n",
        "    elif meta_model_type == 'random_forest':\n",
        "        meta_model = RandomForestClassifier(**best_params)\n",
        "\n",
        "    meta_model.fit(meta_features, y_train)\n",
        "    meta_predictions = meta_model.predict(meta_eval_features)\n",
        "\n",
        "    accuracy = accuracy_score(y_eval, meta_predictions)\n",
        "    precision = precision_score(y_eval, meta_predictions, average='binary')\n",
        "    recall = recall_score(y_eval, meta_predictions, average='binary')\n",
        "    f1 = f1_score(y_eval, meta_predictions, average='binary')\n",
        "\n",
        "    print(f\"Optimized Stacking Ensemble Accuracy: {accuracy}\")\n",
        "    print(f\"Optimized Stacking Ensemble Precision: {precision}\")\n",
        "    print(f\"Optimized Stacking Ensemble Recall: {recall}\")\n",
        "    print(f\"Optimized Stacking Ensemble F1-Score: {f1}\")\n",
        "\n",
        "    metrics_file_path = f\"{save_path}/optuna_stacking_ensemble_metrics.txt\"\n",
        "    with open(metrics_file_path, 'w') as f:\n",
        "        f.write(f\"Optimized Stacking Ensemble Metrics ({meta_model_type}):\\n\")\n",
        "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
        "        f.write(f\"Precision: {precision}\\n\")\n",
        "        f.write(f\"Recall: {recall}\\n\")\n",
        "        f.write(f\"F1-Score: {f1}\\n\")\n",
        "\n",
        "    # Save predictions for final evaluation set\n",
        "    np.save(f\"{save_path}/meta_eval_predictions.npy\", meta_predictions)\n",
        "    np.save(f\"{save_path}/meta_eval_true_labels.npy\", y_eval)\n",
        "\n",
        "    meta_model_path = f\"{save_path}/optuna_stacking_ensemble_model_{meta_model_type}.pkl\"\n",
        "    joblib.dump(meta_model, meta_model_path)\n",
        "    print(f\"Optimized meta-classifier saved to: {meta_model_path}\")\n",
        "    print(f\"Predictions and labels saved for final evaluation set.\")\n"
      ],
      "metadata": {
        "id": "GuKc10lF77Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train in-domain stacking ensemble 1\n",
        "stacking_ensemble_with_optuna(\n",
        "    in_domain_stack_one,\n",
        "    train_data=olid_train_data,\n",
        "    eval_data=olid_test_data,\n",
        "    meta_model_type='xgboost',\n",
        "    save_path=\"/content/drive/MyDrive/results_retrain_opt/in_domain_stack_3\"\n",
        ")\n",
        "\n",
        "# Train cross-domain stacking ensemble 1\n",
        "stacking_ensemble_with_optuna(\n",
        "    cross_domain_stack_one,\n",
        "    train_data=hasoc_train_data,\n",
        "    eval_data=olid_test_data,\n",
        "    meta_model_type='xgboost',\n",
        "    save_path=\"/content/drive/MyDrive/results_retrain_opt/cross_domain_stack_3\"\n",
        ")\n",
        "\n",
        "# Train in-domain stacking ensemble 2\n",
        "stacking_ensemble_with_optuna(\n",
        "    in_domain_stack_two,\n",
        "    train_data=olid_train_data,\n",
        "    eval_data=olid_test_data,\n",
        "    meta_model_type='xgboost',\n",
        "    save_path=\"/content/drive/MyDrive/results_retrain_opt/in_domain_stack_4\"\n",
        ")\n",
        "\n",
        "# Train cross-domain stacking ensemble 2\n",
        "stacking_ensemble_with_optuna(\n",
        "    cross_domain_stack_two,\n",
        "    train_data=hasoc_train_data,\n",
        "    eval_data=olid_test_data,\n",
        "    meta_model_type='xgboost',\n",
        "    save_path=\"/content/drive/MyDrive/results_retrain_opt/cross_domain_stack_4\"\n",
        ")"
      ],
      "metadata": {
        "id": "_oznnkMZeGEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G5Nb4se4OEVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for data processing, model evaluation, and loading transformer models\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define directories and stacks for in-domain and cross-domain models\n",
        "root_dir = \"/content/drive/MyDrive/final_results\"\n",
        "olid_test = pd.read_csv('/content/drive/MyDrive/sem4/sem4/olid-test.csv')\n",
        "olid_test_data = pd.DataFrame({'text': olid_test['text'], 'labels': olid_test['labels']})\n",
        "\n",
        "in_domain_paths = {\n",
        "    \"bert\": \"/content/drive/MyDrive/results/OLID_bert-base-cased_lr_1e-05_bs_16_epochs_5\",\n",
        "    \"roberta\": \"/content/drive/MyDrive/results/OLID_roberta-base_lr_2e-05_bs_16_epochs_5\",\n",
        "    \"hatebert\": \"/content/drive/MyDrive/results/OLID_GroNLP/hateBERT_lr_1e-05_bs_16_epochs_5\",\n",
        "    \"distilbert\": \"/content/drive/MyDrive/results/OLID_distilbert-base-cased_lr_1e-05_bs_16_epochs_5\"\n",
        "}\n",
        "\n",
        "cross_domain_paths = {\n",
        "    \"bert\": \"/content/drive/MyDrive/results/HASOC_bert-base-cased_lr_2e-05_bs_16_epochs_5\",\n",
        "    \"roberta\": \"/content/drive/MyDrive/results/HASOC_roberta-base_lr_1e-05_bs_16_epochs_5\",\n",
        "    \"hatebert\": \"/content/drive/MyDrive/results/HASOC_GroNLP/hateBERT_lr_1e-05_bs_16_epochs_5\",\n",
        "    \"distilbert\": \"/content/drive/MyDrive/results/HASOC_distilbert-base-cased_lr_1e-05_bs_16_epochs_5\"\n",
        "}\n",
        "\n",
        "stacks = {\n",
        "    \"stack_one\": [\"roberta\", \"distilbert\", \"hatebert\"],\n",
        "    \"stack_two\": [\"roberta\", \"bert\", \"hatebert\"],\n",
        "    \"stack_three\": [\"roberta\", \"distilbert\", \"bert\"]\n",
        "}\n",
        "\n",
        "\n",
        "def load_stack_models(model_paths, stack):\n",
        "    models = []\n",
        "    for model_name in stack:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_paths[model_name]).to(\"cuda\")\n",
        "        models.append(model)\n",
        "    return models\n",
        "\n",
        "# Perform majority voting (hard or soft) across models in a stack\n",
        "def majority_voting(texts, tokenizer, models, voting_type=\"hard\", batch_size=16):\n",
        "    all_predictions = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"{voting_type.capitalize()} Voting Prediction Batches\"):\n",
        "        # Encode batch of texts for input to the models\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        encoding = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "        input_ids, attention_mask = encoding['input_ids'], encoding['attention_mask']\n",
        "\n",
        "        batch_predictions = []\n",
        "        combined_probs = None\n",
        "        for model in models:\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "                # Hard voting: collect predictions for each model in the stack\n",
        "                if voting_type == \"hard\":\n",
        "                    preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "                    batch_predictions.append(preds)\n",
        "                # Soft voting: accumulate probabilities for final prediction\n",
        "                elif voting_type == \"soft\":\n",
        "                    probs = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()\n",
        "                    combined_probs = probs if combined_probs is None else combined_probs + probs\n",
        "\n",
        "        # Final decision based on majority voting type\n",
        "        if voting_type == \"hard\":\n",
        "            final_predictions = mode(np.array(batch_predictions), axis=0)[0].flatten()\n",
        "        elif voting_type == \"soft\":\n",
        "            combined_probs /= len(models)\n",
        "            final_predictions = np.argmax(combined_probs, axis=1)\n",
        "\n",
        "        # Store final predictions for the batch\n",
        "        all_predictions.extend(final_predictions)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "def save_results_to_csv(predictions, labels, voting_type, domain, stack_name):\n",
        "    # Calculate classification report and confusion matrix\n",
        "    report = classification_report(labels, predictions, output_dict=True)\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # Structure the results to save in a row\n",
        "    result_row = {\n",
        "        \"Domain\": domain,\n",
        "        \"Voting Type\": voting_type,\n",
        "        \"Stack Type\": stack_name,\n",
        "        \"Accuracy\": report[\"accuracy\"],\n",
        "        \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
        "        \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
        "        \"F1-Score\": report[\"weighted avg\"][\"f1-score\"],\n",
        "        \"True Positives\": tp,\n",
        "        \"False Positives\": fp,\n",
        "        \"True Negatives\": tn,\n",
        "        \"False Negatives\": fn\n",
        "    }\n",
        "\n",
        "    result_file = f\"/content/drive/MyDrive/final_results/voting_ensemble_results.csv\"\n",
        "    if not os.path.exists(result_file):\n",
        "        pd.DataFrame([result_row]).to_csv(result_file, index=False)\n",
        "    else:\n",
        "        pd.DataFrame([result_row]).to_csv(result_file, mode='a', header=False, index=False)\n",
        "\n",
        "# Execute both hard and soft voting for each stack in both in-domain and cross-domain contexts\n",
        "for stack_name, stack_models in stacks.items():\n",
        "    # Load models for in-domain and cross-domain stacks\n",
        "    in_domain_models = load_stack_models(in_domain_paths, stack_models)\n",
        "    cross_domain_models = load_stack_models(cross_domain_paths, stack_models)\n",
        "\n",
        "    # Iterate over each domain and voting type, and save results\n",
        "    for domain, models, tokenizer in [\n",
        "        (\"in_domain\", in_domain_models, AutoTokenizer.from_pretrained(in_domain_paths[\"bert\"])),\n",
        "        (\"cross_domain\", cross_domain_models, AutoTokenizer.from_pretrained(cross_domain_paths[\"bert\"]))\n",
        "    ]:\n",
        "        for voting_type in [\"hard\", \"soft\"]:\n",
        "            predictions = majority_voting(olid_test_data['text'].tolist(), tokenizer, models, voting_type=voting_type)\n",
        "            save_results_to_csv(predictions, olid_test_data['labels'], voting_type, domain, stack_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9fENxi-GGMv",
        "outputId": "b8b165e3-38c1-47c7-8ff8-01595282f79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hard Voting Prediction Batches: 100%|██████████| 54/54 [00:10<00:00,  5.18it/s]\n",
            "Soft Voting Prediction Batches: 100%|██████████| 54/54 [00:10<00:00,  5.22it/s]\n",
            "Hard Voting Prediction Batches: 100%|██████████| 54/54 [00:10<00:00,  5.10it/s]\n",
            "Soft Voting Prediction Batches: 100%|██████████| 54/54 [00:11<00:00,  4.80it/s]\n",
            "Hard Voting Prediction Batches: 100%|██████████| 54/54 [00:13<00:00,  4.08it/s]\n",
            "Soft Voting Prediction Batches: 100%|██████████| 54/54 [00:13<00:00,  4.12it/s]\n",
            "Hard Voting Prediction Batches: 100%|██████████| 54/54 [00:12<00:00,  4.20it/s]\n",
            "Soft Voting Prediction Batches: 100%|██████████| 54/54 [00:12<00:00,  4.28it/s]\n",
            "Hard Voting Prediction Batches: 100%|██████████| 54/54 [00:10<00:00,  5.04it/s]\n",
            "Soft Voting Prediction Batches: 100%|██████████| 54/54 [00:10<00:00,  5.07it/s]\n",
            "Hard Voting Prediction Batches: 100%|██████████| 54/54 [00:10<00:00,  5.03it/s]\n",
            "Soft Voting Prediction Batches: 100%|██████████| 54/54 [00:10<00:00,  4.98it/s]\n"
          ]
        }
      ]
    }
  ]
}